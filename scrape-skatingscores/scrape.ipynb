{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping skatingscores.com\n",
    "\n",
    "The purpose of this notebook is to test and then sucesfully scrape https://skatingscores.com/. Practically, I could ask for the data, but because this is a learning project we will scrape assuming we couldn't get it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapeEnhancedProtocol import *\n",
    "from scrapeEventsPage import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_2122 = scrapeEvents(url = 'https://skatingscores.com/2122/')\n",
    "season_2122.scrape()\n",
    "events_2122 = season_2122.events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: adjust to pull short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LPIDI women, long not scrapable\n",
      "LPIDI men, long not scrapable\n",
      "JGPFRA women, long not scrapable\n",
      "JGPFRA men, long not scrapable\n",
      "JGPFRA2 women, long not scrapable\n",
      "JGPFRA2 men, long not scrapable\n",
      "JGPSVK women, long not scrapable\n",
      "JGPSVK men, long not scrapable\n",
      "JNPCHA women, long not scrapable\n",
      "JNPCHA men, long not scrapable\n",
      "JGPRUS women, long not scrapable\n",
      "JGPRUS men, long not scrapable\n"
     ]
    }
   ],
   "source": [
    "## LONG\n",
    "DIVISION = ['sr']\n",
    "SEX = ['women', 'men']\n",
    "PROGRAM = ['long']\n",
    "\n",
    "def main(event_name, website):\n",
    "    long = scrapeEvent(event_name = event_name, website = website, long = True)\n",
    "\n",
    "    long.get_component_score()\n",
    "    LONG_ALL_COMPONENTS.append(long.all_components)\n",
    "    \n",
    "    long.get_technical_score()\n",
    "    LONG_ALL_TECHNICAL.append(long.all_technical)\n",
    "\n",
    "    LONG_ALL_SKATER.append(long.all_skaters)\n",
    "\n",
    "\n",
    "LONG_ALL_COMPONENTS = []\n",
    "LONG_ALL_TECHNICAL = []\n",
    "LONG_ALL_SKATER = []\n",
    "for index, row in events_2122[['Event_Abbreviation','URL']].iterrows():\n",
    "    for s in SEX:\n",
    "        for p in PROGRAM:\n",
    "            ending = f'sr/{s}/{p}'\n",
    "            try:\n",
    "                \n",
    "                main(event_name = row['Event_Abbreviation'],website = row['URL'] + ending)\n",
    "                time.sleep(0.33) # be nice to the generous server admins!\n",
    "            \n",
    "            except Exception as e:\n",
    "\n",
    "                #errors can occur because event got cancelled (from the panini) or event may be juniors only (out of scope)\n",
    "                print(row['Event_Abbreviation'], f\"{s}, {p} not scrapable\")\n",
    "                pass\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## SHORT\n",
    "# DIVISION = ['sr']\n",
    "# SEX = ['women', 'men']\n",
    "# PROGRAM = ['short']\n",
    "\n",
    "\n",
    "\n",
    "# SHORT_ALL_COMPONENTS = []\n",
    "# SHORT_ALL_TECHNICAL = []\n",
    "# SHORT_ALL_SKATER = []\n",
    "\n",
    "# def main(event_name, website):\n",
    "#     short = scrapeEvent(event_name = event_name, website = website, long = False)\n",
    "#     short.get_component_score()\n",
    "#     SHORT_ALL_COMPONENTS.append(short.all_components)\n",
    "#     short.get_technical_score()\n",
    "#     SHORT_ALL_TECHNICAL.append(short.all_technical)\n",
    "#     SHORT_ALL_SKATER.append(event.all_skaters)\n",
    "\n",
    "\n",
    "# for index, row in events_2122[['Event_Abbreviation','URL']].iterrows():\n",
    "#     for s in SEX:\n",
    "#         for p in PROGRAM:\n",
    "#             ending = f'sr/{s}/{p}'\n",
    "#             try:\n",
    "                \n",
    "#                 main(event_name = row['Event_Abbreviation'],website = row['URL'] + ending)\n",
    "#                 time.sleep(0.33) # be nice to the generous server admins!\n",
    "            \n",
    "#             except Exception as e:\n",
    "\n",
    "#                 #errors can occur because event got cancelled (from the panini) or event may be juniors only (out of scope)\n",
    "#                 print(row['Event_Abbreviation'], f\"{s}, {p} not scrapable\")\n",
    "#                 #print(e)\n",
    "#                 pass\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_concat(list_of_dfs):\n",
    "    \"\"\"Concat iteratively inorder to drop duplicates and avoid InvalidIndexError\"\"\"\n",
    "    start = list_of_dfs[0]\n",
    "    for i in list_of_dfs[1:]:\n",
    "        try:\n",
    "            start = pd.concat([start, i.reset_index(drop = True)], axis = 0)\n",
    "        except Exception as e:\n",
    "            print(f\"dropped {i.head(1)}\")\n",
    "            pass\n",
    "    return start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = iterative_concat(ALL_COMPONENTS)\n",
    "components.to_csv('../data/skatingScores/components.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "technicals = iterative_concat(ALL_TECHNICAL)\n",
    "technicals.to_csv('../data/skatingScores/technicals.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skaters = iterative_concat(ALL_SKATER)\n",
    "skaters.to_csv('../data/skatingScores/skaters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Appendix: more data about the type of jumps\n",
    "isu_wiki = pd.read_html('https://en.wikipedia.org/wiki/ISU_Judging_System')\n",
    "jumps_and_code = isu_wiki[4]\n",
    "#jumps_and_code.columns = [i[0] for i in jumps_and_code.columns]\n",
    "#display(liu_cran, jumps_and_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad4d327c06e4004b7975721c179ad2df204bf3c26d8310c3710ad77225acbaec"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
